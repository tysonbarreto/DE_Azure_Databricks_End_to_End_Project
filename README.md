
# âš™ï¸ Azure Data Engineering Pipeline

A modular, production-grade data engineering solution combining **Azure Data Factory** for ETL and **Azure Databricks** for real-time streaming and analytics. This project demonstrates scalable, fault-tolerant data movement and transformation using modern cloud-native tools.

---

## ğŸ“Œ Project Overview

This pipeline consists of two integrated components:

### 1ï¸âƒ£ Azure Data Factory â€“ Incremental ETL with CDC
- Implements **Change Data Capture (CDC)** for multiple source tables  
- Performs **conditional update checks** and **incremental loads**  
- Integrates **Logic Apps** for failure notifications and alerting  
- Ensures data freshness and operational transparency

### 2ï¸âƒ£ Azure Databricks â€“ Streaming & Lakehouse Processing
- Streams ingested data from **ADLS Gen2** using **Delta Lake**  
- Utilizes **LakeFlow** and **AutoLoader** for declarative ingestion  
- Applies **checkpointing** and **idempotency** for reliable processing  
- Uses **Jinja templating** for dynamic pipeline configuration  
- Computes real-time metrics and supports downstream analytics

---

## ğŸ—‚ï¸ Folder Structure

```
project-root/
â”œâ”€â”€ adf/
â”‚   â””â”€â”€ cdc_pipeline.json         # ADF pipeline definition with CDC logic
â”‚   â””â”€â”€ logic_app_alerts.json     # Logic App for failure notifications
â”œâ”€â”€ databricks/
â”‚   â””â”€â”€ lakeflow_ingestion.py     # Declarative streaming with AutoLoader
â”‚   â””â”€â”€ jinja_templates/          # Configurable templates for pipeline logic
â”‚   â””â”€â”€ checkpoint_config.json    # Checkpointing and idempotency setup
```

---

## ğŸš€ Key Features

- âœ… Incremental ETL with CDC  
- ğŸ” Stream ingestion with AutoLoader  
- ğŸ§© Config-driven pipelines via Jinja  
- ğŸ›¡ï¸ Idempotent and fault-tolerant design  
- ğŸ“£ Alerting via Logic Apps  
- ğŸ“Š Delta Lake for scalable analytics

---

## ğŸ§ª Prerequisites

- Azure Data Factory with access to source systems  
- Azure Logic Apps for alerting  
- Azure Databricks workspace with Unity Catalog  
- ADLS Gen2 configured for Delta Lake storage  
- Python 3.8+ and required Databricks libraries

---

## ğŸ“ˆ Next Steps

- Extend CDC logic to additional domains  
- Integrate with Power BI or Azure Synapse for reporting  
- Add data quality checks and lineage tracking  
- Enable schema evolution and audit logging
